{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb19eba-5856-4bbe-afd6-25ebc5739531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import httpx\n",
    "import asyncio\n",
    "import time\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67755ffc-40fc-40f3-b113-c0a9b8367ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройки\n",
    "DATA_PATH = Path('../data/raw/html')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "METADATA_PATH = '../data/metadata/arxiv_NLP_2025_metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f46b72-f5da-4841-b6ad-0cd7a761618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "df = pd.read_csv(METADATA_PATH, usecols=['arxiv_id', 'title', 'html_url'])\n",
    "print(f'Данные загружены. Всего {df.shape[0]} статей.')\n",
    "\n",
    "ids = df['arxiv_id'].to_list()\n",
    "ids = set(ids)\n",
    "\n",
    "downloaded_ids = set()\n",
    "for f in DATA_PATH.glob('*.html'):\n",
    "    downloaded_ids.add(f.stem)\n",
    "\n",
    "ids_to_download = df[~df['arxiv_id'].isin(downloaded_ids)]\n",
    "print(f'Скачанных статей: {len(downloaded_ids)}. Осталось статей: {ids_to_download.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e673d615-3c4e-4e24-b37e-5c28d9b8bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMAPHORE = asyncio.Semaphore(10)\n",
    "\n",
    "async def download_article(client, row, pbar):\n",
    "    async with SEMAPHORE:\n",
    "        try:\n",
    "            response = await client.get(row.html_url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                paper_id = row.arxiv_id\n",
    "                filename = DATA_PATH / f'{paper_id}.html'\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        finally:\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364c494-ba3e-41c0-907a-a956e3014d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    limits = httpx.Limits(max_keepalive_connections=10, max_connections=20)\n",
    "    \n",
    "    async with httpx.AsyncClient(limits=limits, timeout=20.0, follow_redirects=True) as client:\n",
    "        tasks = []\n",
    "        \n",
    "        pbar = tqdm(total=len(ids_to_download))\n",
    "        \n",
    "        for row in ids_to_download.itertuples():\n",
    "            task = download_article(client, row, pbar)\n",
    "            tasks.append(task)\n",
    "        \n",
    "        await asyncio.gather(*tasks)\n",
    "        pbar.close()\n",
    "\n",
    "await main()\n",
    "\n",
    "downloaded_ids = set()\n",
    "for f in DATA_PATH.glob('*.html'):\n",
    "    downloaded_ids.add(f.stem)\n",
    "\n",
    "ids_to_download = df[~df['arxiv_id'].isin(downloaded_ids)]\n",
    "len_all = len(ids)\n",
    "len_dwd = len(downloaded_ids)\n",
    "not_dwd_pct = ((len_all - len_dwd) / len_all) * 100\n",
    "print(f'Скачано {len_dwd} из {len_all} статей. Процент нескачанных статей: {not_dwd_pct:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef639a-11b8-4a61-9b32-7658de6fc2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d917e4-dfdc-47ef-936a-3c3748450f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_THRESHOLD = 20 * 1024\n",
    "\n",
    "for file in DATA_PATH.glob('*.html'):\n",
    "    if file.is_file():\n",
    "        # Сначала получаем размер\n",
    "        file_size = file.stat().st_size \n",
    "        \n",
    "        if file_size < SIZE_THRESHOLD:\n",
    "            try:\n",
    "                file.unlink()\n",
    "                print(f\"Удален: {file.name} ({file_size} bytes)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при удалении {file.name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27769f-88dc-4e0b-b40f-7f42f0b7fbea",
   "metadata": {},
   "source": [
    "# Собираем Parquet для хранения в S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "929f6eb0-0a00-4974-92c2-62b72c574ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f47db3-c18c-4844-8bc4-4d85f3300946",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_DIR = Path('../data/raw/html').resolve()\n",
    "DEST_DIR = Path(\"../data/raw/parquet\").resolve()\n",
    "DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SHARD_SIZE = 1000\n",
    "\n",
    "html_files = list(SOURCE_DIR.glob(\"*.html\"))\n",
    "total_shards = math.ceil(len(html_files) / SHARD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e76d183-fe3e-478b-8d81-b38970ac0f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(files, size):\n",
    "    \"\"\"Генератор чанков из списка файлов\"\"\"\n",
    "    for i in range(0, len(files), size):\n",
    "        yield files[i:i + size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c61721-8429-4a7d-9d94-e11a8e1c57e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8248162f44584ed09ec53380dea69bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for shard_id, batch in enumerate(tqdm(get_batches(html_files, SHARD_SIZE), desc=\"Packing\", total=total_shards)):\n",
    "    data = {\n",
    "        \"doc_id\": [f.stem for f in batch],\n",
    "        \"html\": [f.read_text(encoding=\"utf-8\", errors=\"replace\") for f in batch],\n",
    "        \"source_path\": [str(f) for f in batch]\n",
    "    }\n",
    "    \n",
    "    table = pa.Table.from_pydict(data)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        DEST_DIR / f\"shard_{shard_id:04d}.parquet\",\n",
    "        compression=\"zstd\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280b45c-5460-4f3e-81fb-223d2e817712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112d7fa-f39f-4ba6-b08a-cf60b771649e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
