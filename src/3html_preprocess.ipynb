{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79912dbc-184b-46c7-9d1a-20d8da421b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from markdownify import markdownify as md\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6750e22-b99c-48d1-8ad7-088f60071c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_arxiv_html(html_text):\n",
    "    if \"Conversion to HTML had a Fatal error\" in html_text:\n",
    "        return None\n",
    "        \n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    article = soup.find('article') or soup.find(class_='ltx_page_content')\n",
    "    \n",
    "    if not article:\n",
    "        return None\n",
    "\n",
    "    # Удаляем References\n",
    "    refs_section = soup.find('section', class_='ltx_bibliography')\n",
    "    if refs_section:\n",
    "        refs_section.decompose()\n",
    "    # Удаляем Acknowledgements\n",
    "    header = soup.find(['h2', 'h3'], string=re.compile(r'^\\s*Acknowledgements?\\s*$', re.IGNORECASE))\n",
    "    if header:\n",
    "        section = header.find_parent('section')\n",
    "        if section:\n",
    "            section.decompose()\n",
    "    # Удаляем авторов\n",
    "    authors_section = soup.find('div', class_='ltx_authors')\n",
    "    if authors_section:\n",
    "        authors_section.decompose()\n",
    "    # Удаляем footnotetext\n",
    "    fnt = soup.find('span', class_='ltx_role_footnotetext')\n",
    "    if fnt:\n",
    "        fnt.decompose()\n",
    "    # Удаляем ошибки\n",
    "    error_section = soup.find('span', class_='ltx_ERROR')\n",
    "    if error_section:\n",
    "        section = error_section.find_parent('div')\n",
    "        if section:\n",
    "            section.decompose()\n",
    "        \n",
    "    \n",
    "    math_registry = {}\n",
    "    for i, math in enumerate(article.find_all(class_='ltx_Math')):\n",
    "        latex = math.get('alttext', '')\n",
    "        if latex:\n",
    "            placeholder = f\"MATHITEM{i}END\" \n",
    "            math_registry[placeholder] = f\"${latex}$\"\n",
    "            math.replace_with(f\" {placeholder} \")\n",
    "\n",
    "    markdown_text = md(str(article), heading_style=\"ATX\")\n",
    "\n",
    "    for placeholder, original_latex in math_registry.items():\n",
    "        markdown_text = markdown_text.replace(placeholder, original_latex)\n",
    "\n",
    "    return markdown_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4973e8-d4cd-4af0-8a27-2030199419f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../data/raw/html/')\n",
    "\n",
    "PROCESSED_DATA_DIR = Path('../data/processed/md/')\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31eb387c-7e2f-4d0c-b174-3987bbb9e8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DATA_DIR.glob('*.html')\n",
    "\n",
    "data = [f for f in data if f.stat().st_size >= 50 * 1024] # не обрабатываем файлы меньше 50 Кб\n",
    "data = list(data)[:100] ############################# ТУТ МЕНЯЕМ КОЛИЧЕСТВО СТАТЕЙ ДЛЯ ТЕСТОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7e86b6-c19d-4344-aea4-55e2d837697b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f1ca793a5b4f2da069d71bade3b793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for html_file in tqdm(data, total=len(data)):\n",
    "    try:\n",
    "        html_content = html_file.read_text(encoding='utf-8')\n",
    "        md_content = preprocess_arxiv_html(html_content)\n",
    "        filename = f'{PROCESSED_DATA_DIR / html_file.stem}.md'\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(md_content)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c342bd-a511-4f9e-98c4-10139bc9a627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad8f017b-41cc-4e0c-a721-8e9bf9a83539",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = list(PROCESSED_DATA_DIR.glob('*.md'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fafc1891-461c-4393-a34b-bee7ed4d530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.|2501.00691v2|\n",
      " # Labels Generated by Large Language Models Help M\n",
      "======================================================================\n",
      "2.|2501.00697v2|\n",
      " # PANDA – Paired Anti-hate Narratives Dataset from\n",
      "======================================================================\n",
      "3.|2501.00713v2|\n",
      " # CODEOFCONDUCT at Multilingual Counterspeech Gene\n",
      "======================================================================\n",
      "4.|2501.00715v1|\n",
      " # eRevise+RF: A Writing Evaluation System for Asse\n",
      "======================================================================\n",
      "5.|2501.00733v1|\n",
      " # On Importance of Layer Pruning for Smaller BERT \n",
      "======================================================================\n",
      "6.|2501.00745v2|\n",
      " # Dynamics of Adversarial Attacks on Large Languag\n",
      "======================================================================\n",
      "7.|2501.00747v1|\n",
      " # DIVE: Diversified Iterative Self-Improvement\n",
      "\n",
      "##\n",
      "======================================================================\n",
      "8.|2501.00759v3|\n",
      " # Enhancing Transformers for Generalizable First-O\n",
      "======================================================================\n",
      "9.|2501.00777v3|\n",
      " # FitCF: A Framework for Automatic Feature Importa\n",
      "======================================================================\n",
      "10.|2501.00778v1|\n",
      " # Decoding the Flow: CauseMotion for Emotional Cau\n",
      "======================================================================\n",
      "11.|2501.00782v1|\n",
      " # Navigating Nuance: In Quest for Political Truth\n",
      "\n",
      "======================================================================\n",
      "12.|2501.00803v1|\n",
      " # Reasoning-Oriented and Analogy-Based Methods for\n",
      "======================================================================\n",
      "13.|2501.00828v1|\n",
      " # Embedding Style Beyond Topics: Analyzing Dispers\n",
      "======================================================================\n",
      "14.|2501.00830v2|\n",
      " # LLM+AL: Bridging Large Language Models and Actio\n",
      "======================================================================\n",
      "15.|2501.00862v1|\n",
      " # DiffETM: Diffusion Process Enhanced Embedded Top\n",
      "======================================================================\n",
      "16.|2501.00865v1|\n",
      " # Negative Co-learning to Positive Co-learning wit\n",
      "======================================================================\n",
      "17.|2501.00868v1|\n",
      " # Large Language Models Are Read/Write Policy-Make\n",
      "======================================================================\n",
      "18.|2501.00874v3|\n",
      " # LUSIFER: Language Universal Space Integration fo\n",
      "======================================================================\n",
      "19.|2501.00888v1|\n",
      " # Unfolding the Headline: Iterative Self-Questioni\n",
      "======================================================================\n",
      "20.|2501.00953v2|\n",
      " # Prior Lessons of Incremental Dialogue and Robot \n",
      "======================================================================\n",
      "21.|2501.00982v2|\n",
      " <>\n",
      "======================================================================\n",
      "22.|2501.00999v2|\n",
      " # Exploring Information Processing in Large Langua\n",
      "======================================================================\n",
      "23.|2501.01014v1|\n",
      " # MDSF: Context-Aware Multi-Dimensional Data Story\n",
      "======================================================================\n",
      "24.|2501.01028v4|\n",
      " # KaLM-Embedding: Superior Training Data Brings A \n",
      "======================================================================\n",
      "25.|2501.01031v3|\n",
      " # ValuesRAG: Enhancing Cultural Alignment Through \n",
      "======================================================================\n",
      "26.|2501.01034v2|\n",
      " <>\n",
      "======================================================================\n",
      "27.|2501.01039v1|\n",
      " # MSWA: Refining Local Attention with Multi-Scale \n",
      "======================================================================\n",
      "28.|2501.01046v3|\n",
      " # FED: Fast and Efficient Dataset Deduplication Fr\n",
      "======================================================================\n",
      "29.|2501.01054v1|\n",
      " # Dynamic Scaling of Unit Tests for Code Reward Mo\n",
      "======================================================================\n",
      "30.|2501.01056v1|\n",
      " # Risks of Cultural Erasure in Large Language Mode\n",
      "======================================================================\n",
      "31.|2501.01059v2|\n",
      " # Dynamic Attention-Guided Context Decoding for Mi\n",
      "======================================================================\n",
      "32.|2501.01123v1|\n",
      " # TED: Turn Emphasis with Dialogue Feature Attenti\n",
      "======================================================================\n",
      "33.|2501.01144v5|\n",
      " # BlockDialect: Block-wise Fine-grained Mixed Form\n",
      "======================================================================\n",
      "34.|2501.01158v2|\n",
      " 11institutetext: \n",
      "Monash University, Malaysia 11em\n",
      "======================================================================\n",
      "35.|2501.01168v2|\n",
      " # *Blind Men and the Elephant*: Diverse Perspectiv\n",
      "======================================================================\n",
      "36.|2501.01195v1|\n",
      " # Data Augmentation Techniques for Chinese Disease\n",
      "======================================================================\n",
      "37.|2501.01237v2|\n",
      " <>\n",
      "======================================================================\n",
      "38.|2501.01246v1|\n",
      " # Large Language Model-Enhanced Symbolic Reasoning\n",
      "======================================================================\n",
      "39.|2501.01256v1|\n",
      " # Digital Guardians: Can GPT-4, Perspective API, a\n",
      "======================================================================\n",
      "40.|2501.01257v2|\n",
      " # CodeElo: Benchmarking Competition-level Code Gen\n",
      "======================================================================\n",
      "41.|2501.01273v1|\n",
      " # Does a Large Language Model Really Speak in Huma\n",
      "======================================================================\n",
      "42.|2501.01284v2|\n",
      " # Tracing Partisan Bias to Its Emotional Fingerpri\n",
      "======================================================================\n",
      "43.|2501.01305v1|\n",
      " # Exploring The Potential of Large Language Models\n",
      "======================================================================\n",
      "44.|2501.01306v2|\n",
      " Technical Report on Slow Thinking with LLMs: Hallu\n",
      "======================================================================\n",
      "45.|2501.01332v1|\n",
      " # Decoding Knowledge in Large Language Models: A F\n",
      "======================================================================\n",
      "46.|2501.01336v1|\n",
      " # Aligning Large Language Models for Faithful Inte\n",
      "======================================================================\n",
      "47.|2501.01377v2|\n",
      " # Improving Medical Large Vision-Language Models w\n",
      "======================================================================\n",
      "48.|2501.01384v1|\n",
      " # OmniChat: Enhancing Spoken Dialogue Systems with\n",
      "======================================================================\n",
      "49.|2501.01588v1|\n",
      " # Fine-Tuning PHI-3 for Multiple-Choice Question A\n",
      "======================================================================\n",
      "50.|2501.01594v1|\n",
      " <>\n",
      "======================================================================\n",
      "51.|2501.01625v1|\n",
      " # ICPC: In-context Prompt Compression with Faster \n",
      "======================================================================\n",
      "52.|2501.01638v2|\n",
      " # A non-ergodic framework for understanding emerge\n",
      "======================================================================\n",
      "53.|2501.01644v2|\n",
      " # Multimodal Contrastive Representation Learning i\n",
      "======================================================================\n",
      "54.|2501.01652v2|\n",
      " # MIRAGE: Exploring How Large Language Models Perf\n",
      "======================================================================\n",
      "55.|2501.01668v2|\n",
      " # CoT-based Synthesizer: Enhancing LLM Performance\n",
      "======================================================================\n",
      "56.|2501.01679v1|\n",
      " # Adaptive Few-shot Prompting for Machine Translat\n",
      "======================================================================\n",
      "57.|2501.01705v2|\n",
      " # *The Essence of Contextual Understanding in Theo\n",
      "======================================================================\n",
      "58.|2501.01743v3|\n",
      " # Automating Legal Interpretation with LLMs: Retri\n",
      "======================================================================\n",
      "59.|2501.01796v1|\n",
      " # Reading Between the Lines: A dataset and a study\n",
      "======================================================================\n",
      "60.|2501.01805v2|\n",
      " # End-to-End Long Document Summarization using Gra\n",
      "======================================================================\n",
      "61.|2501.01827v1|\n",
      " # The Proof is in the Almond Cookies\n",
      "\n",
      "A Case Study\n",
      "======================================================================\n",
      "62.|2501.01832v1|\n",
      " # Time Series Language Model for Descriptive Capti\n",
      "======================================================================\n",
      "63.|2501.01872v6|\n",
      " # Turning Logic Against Itself: Probing Model Defe\n",
      "======================================================================\n",
      "64.|2501.01956v3|\n",
      " # Metadata Conditioning Accelerates Language Model\n",
      "======================================================================\n",
      "65.|2501.02009v2|\n",
      " # Cross-model Transferability among Large Language\n",
      "======================================================================\n",
      "66.|2501.02018v1|\n",
      " # Safeguarding Large Language Models in Real-time \n",
      "======================================================================\n",
      "67.|2501.02020v3|\n",
      " # Enhancing Uncertainty Modeling with Semantic Gra\n",
      "======================================================================\n",
      "68.|2501.02026v1|\n",
      " # Recursive Decomposition of Logical Thoughts: Fra\n",
      "======================================================================\n",
      "69.|2501.02031v1|\n",
      " # CarbonChat: Large Language Model-Based Corporate\n",
      "======================================================================\n",
      "70.|2501.02039v3|\n",
      " # An Investigation into Value Misalignment in LLM-\n",
      "======================================================================\n",
      "71.|2501.02068v3|\n",
      " # The interplay between domain specialization and \n",
      "======================================================================\n",
      "72.|2501.02086v3|\n",
      " # Instruction-Following Pruning for Large Language\n",
      "======================================================================\n",
      "73.|2501.02090v1|\n",
      " # Applying Text Mining to Analyze Human Question A\n",
      "======================================================================\n",
      "74.|2501.02157v2|\n",
      " # Personalized Graph-Based Retrieval for Large Lan\n",
      "======================================================================\n",
      "75.|2501.02196v1|\n",
      " 11institutetext: \n",
      "Peking University, Beijing, Chin\n",
      "======================================================================\n",
      "76.|2501.02235v2|\n",
      " # Survey on Question Answering over Visually Rich \n",
      "======================================================================\n",
      "77.|2501.02237v1|\n",
      " # Financial Named Entity Recognition: How Far Can \n",
      "======================================================================\n",
      "78.|2501.02266v1|\n",
      " # LLMzSzŁ: a comprehensive LLM benchmark for Polis\n",
      "======================================================================\n",
      "79.|2501.02295v4|\n",
      " # Explicit vs. Implicit: Investigating Social Bias\n",
      "======================================================================\n",
      "80.|2501.02336v1|\n",
      " # AdaSkip: Adaptive Sublayer Skipping for Accelera\n",
      "======================================================================\n",
      "81.|2501.02361v1|\n",
      " [1]\\fnmÇağrı \\surSayallar\n",
      "\n",
      "[1]\\orgdivComputer Engi\n",
      "======================================================================\n",
      "82.|2501.02370v3|\n",
      " # Prepending or Cross-Attention for Speech-to-Text\n",
      "======================================================================\n",
      "83.|2501.02407v2|\n",
      " # Towards the Anonymization of the Language Modeli\n",
      "======================================================================\n",
      "84.|2501.02432v1|\n",
      " # Swift Cross-Dataset Pruning: Enhancing Fine-Tuni\n",
      "======================================================================\n",
      "85.|2501.02434v1|\n",
      " # Towards Multimodal Metaphor Understanding: A Chi\n",
      "======================================================================\n",
      "86.|2501.02448v2|\n",
      " # Understand, Solve and Translate: Bridging the Mu\n",
      "======================================================================\n",
      "87.|2501.02460v3|\n",
      " # Towards Omni-RAG: Comprehensive Retrieval-Augmen\n",
      "======================================================================\n",
      "88.|2501.02471v2|\n",
      " # Hengqin-RA-v1: Advanced Large Language Model for\n",
      "======================================================================\n",
      "89.|2501.02482v1|\n",
      " # Decoding News Bias: Multi Bias Detection in News\n",
      "======================================================================\n",
      "90.|2501.02506v4|\n",
      " # ToolHop: A Query-Driven Benchmark for Evaluating\n",
      "======================================================================\n",
      "91.|2501.02511v1|\n",
      " # Can Impressions of Music be Extracted from Thumb\n",
      "======================================================================\n",
      "92.|2501.02518v2|\n",
      " # CHAIR-Classifier of Hallucination As Improver\n",
      "\n",
      "#\n",
      "======================================================================\n",
      "93.|2501.02552v1|\n",
      " 11institutetext: Teamreboott Inc., Busan, Korea\n",
      "  \n",
      "======================================================================\n",
      "94.|2501.02598v1|\n",
      " # GIT-CXR: End-to-End Transformer for Chest X-Ray \n",
      "======================================================================\n",
      "95.|2501.02599v1|\n",
      " # Empowering Bengali Education with AI: Solving Be\n",
      "======================================================================\n",
      "96.|2501.02631v1|\n",
      " # Prune or Retrain: Optimizing the Vocabulary of M\n",
      "======================================================================\n",
      "97.|2501.02654v2|\n",
      " # Tougher Text, Smarter Models: Raising the Bar fo\n",
      "======================================================================\n",
      "98.|2501.02702v1|\n",
      " # QuIM-RAG: Advancing Retrieval-Augmented Generati\n",
      "======================================================================\n",
      "99.|2501.02739v1|\n",
      " # TARDiS : Text Augmentation for Refining Diversit\n",
      "======================================================================\n",
      "100.|2501.02790v1|\n",
      " # Segmenting Text and Learning Their Rewards for I\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "for i, file in enumerate(data_processed):\n",
    "    content = file.read_text(encoding='utf-8')\n",
    "    print(f'{i+1}.|{file.stem}|\\n {content[:50]}', end=f'\\n{\"=\"*70}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d8f50-e31c-43ad-883e-5b60fe6be3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
