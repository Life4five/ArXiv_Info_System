{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62744b32-d238-449a-9faf-9bf0cb775ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1bfdde-0504-443d-a630-13a023a47595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подключаемся к локальному Qdrant\n",
    "client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "# КОНФИГ\n",
    "COLLECTION_NAME = \"nlp2025_chunks\"\n",
    "VECTOR_SIZE = 1024\n",
    "BATCH_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28dc14ef-a657-446b-991c-d151ccda951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Коллекция 'nlp2025_chunks' успешно создана! Готовы к заливке.\n"
     ]
    }
   ],
   "source": [
    "# 1. Проверяем и удаляем, если уже есть (чтобы начать с чистого листа)\n",
    "if client.collection_exists(collection_name=COLLECTION_NAME):\n",
    "    client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "    print(f\"Коллекция '{COLLECTION_NAME}' удалена.\")\n",
    "\n",
    "# 2. Создаем заново\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(\n",
    "        size=VECTOR_SIZE, \n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "print(f\"Коллекция '{COLLECTION_NAME}' успешно создана! Готовы к заливке.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fa2c7eb-edf5-4fa8-8b11-e5e4e793ddc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Читаем Parquet файл...\n",
      "Загружено строк: 767801\n",
      "Начинаем заливку в Qdrant...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5753eee6ba424dbf03c538483f5187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading:   0%|          | 0/767801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Заливка завершена!\n"
     ]
    }
   ],
   "source": [
    "# Путь к файлу\n",
    "parquet_file = \"../data/processed/all_chunks_with_embeddings.parquet\"\n",
    "\n",
    "print(\"Читаем Parquet файл...\")\n",
    "df = pd.read_parquet(parquet_file)\n",
    "print(f\"Загружено строк: {len(df)}\")\n",
    "\n",
    "points_batch = []\n",
    "total_records = len(df)\n",
    "\n",
    "# Итерируемся по датафрейму\n",
    "print(\"Начинаем заливку в Qdrant...\")\n",
    "for i, row in tqdm(df.iterrows(), total=total_records, desc=\"Uploading\"):\n",
    "    \n",
    "    # Конвертация numpy array -> list float32\n",
    "    # Это критично, чтобы не словить ошибку сериализации JSON\n",
    "    vector = row[\"embedding\"].astype(np.float32).tolist()\n",
    "    \n",
    "    # Сборка метаданных\n",
    "    payload = {\n",
    "        \"text\": row[\"text\"],\n",
    "        \"source\": row[\"source_path\"],\n",
    "        # Распаковка остальных метаданных, если есть\n",
    "        **(row[\"metadata\"] if row[\"metadata\"] else {}) \n",
    "    }\n",
    "\n",
    "    # Добавляем точку в буфер\n",
    "    points_batch.append(\n",
    "        PointStruct(\n",
    "            id=i,  # Используем индекс DataFrame как ID\n",
    "            vector=vector,\n",
    "            payload=payload\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Если буфер заполнился — отправляем\n",
    "    if len(points_batch) >= BATCH_SIZE:\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=points_batch,\n",
    "            wait=False # Не ждем подтверждения записи на диск (быстрее)\n",
    "        )\n",
    "        points_batch = [] # Очищаем буфер\n",
    "\n",
    "# Дозаливаем остатки, если буфер не пуст\n",
    "if points_batch:\n",
    "    client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=points_batch,\n",
    "        wait=True # Тут ждем, чтобы убедиться, что всё закончилось\n",
    "    )\n",
    "\n",
    "print(\"Заливка завершена!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076d401-5403-47cc-9884-a2f7b7762f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21bd2ebc-404b-47fc-b144-80303fbca85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Тестовый поиск (Top-3):\n",
      "ID: 0, Score: 1.0\n",
      "Payload: {'text': 'Labels Generated by Large Language Models Help Measure People’s Empathy in Vitro: ###### Abstract  \\nLarge language models (LLMs) have revolutionised many fields, with LLM-as-a-service (LLMSaaS) offering accessible, general-purpose solutions without costly task-specific training. In contrast to the widely studied prompt engineering for directly solving tasks (in vivo), this paper explores LLMs’ potential for in-vitro applications: using LLM-generated labels to improve supervised training of mainstream models. We examine two strategies – (1) noisy label correction and (2) training data augmentation – in empathy computing, an emerging task to predict psychology-based questionnaire outcomes from inputs like textual narratives. Crowdsourced datasets in this domain often suffer from noisy labels that misrepresent underlying empathy. We show that replacing or supplementing these crowdsourced labels with LLM-generated labels, developed using psychology-based scale-aware prompts, achieves statistically significant accuracy improvements. Notably, the RoBERTa pre-trained language model (PLM) trained with noise-reduced labels yields a state-of-the-art Pearson correlation coefficient of 0.648 on the public NewsEmp benchmarks. This paper further analyses evaluation metric selection and demographic biases to help guide the future development of more equitable empathy computing models. Code and LLM-generated labels are available at <https://github.com/hasan-rakibul/LLMPathy>.  \\n###### Index Terms:  \\nEmpathy detection, Large language model, Natural language processing, Label noise, NewsEmp.  \\n††publicationid: pubid: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice.', 'source': 'D:\\\\GitHub\\\\ArXiv_Info_System\\\\data\\\\raw\\\\html\\\\2501.00691v2.html', 'Header_1': 'Labels Generated by Large Language Models Help Measure People’s Empathy in Vitro', 'Header_2': None, 'Header_3': None}\n",
      "---\n",
      "ID: 12, Score: 0.89048314\n",
      "Payload: {'text': 'Labels Generated by Large Language Models Help Measure People’s Empathy in Vitro > III Method > III-B Applications of Large Language Model in-Vitro: ![Refer to caption](x3.png)  \\nFigure 2: Overview of our proposed in-vitro applications of large language models (LLMs) for enhancing textual empathy prediction with pre-trained language models (PLMs). Application 1 involves correcting noisy labels in an existing dataset using an LLM. Application 2 utilises an LLM to label additional text data, which is then added to the existing training dataset.  \\nOur proposed framework leverages LLM for empathy prediction, as illustrated in [Fig.2](https://arxiv.org/html/2501.00691v2#S3.F2 \"In III-B Applications of Large Language Model in-Vitro ‣ III Method ‣ Labels Generated by Large Language Models Help Measure People’s Empathy in Vitro\"). The first application reduces label noise, while the second application increases the amount of training data by incorporating additional labelled data from LLM. Improved training data from these two applications is fed to a pre-trained language model (PLM) for final empathy prediction.  \\n#### III-B1 Prompt Design  \\nTABLE I: Plain vs scale-aware prompt templates to generate labels from LLM.  \\n| Scheme | System prompt | User prompt |\\n| --- | --- | --- |\\n| Plain | Your task is to measure the empathy of individuals based on their written essays.  Human subjects wrote these essays after reading a newspaper article involving harm to individuals, groups of people, nature, etc. The essay is provided to you within triple backticks. | Essay: ```{essay}```  Now, provide empathy score between 1.0 and 7.0, where a score of 1.0 means the lowest empathy, and a score of 7.0 means the highest empathy.  You must not provide any other outputs apart from the scores |', 'source': 'D:\\\\GitHub\\\\ArXiv_Info_System\\\\data\\\\raw\\\\html\\\\2501.00691v2.html', 'Header_1': 'Labels Generated by Large Language Models Help Measure People’s Empathy in Vitro', 'Header_2': 'III Method', 'Header_3': 'III-B Applications of Large Language Model in-Vitro'}\n",
      "---\n",
      "ID: 52, Score: 0.8818127\n",
      "Payload: {'text': 'Labels Generated by Large Language Models Help Measure People’s Empathy in Vitro > V Conclusion: This work demonstrates the potential of large language models (LLMs) in addressing challenges in empathy computing through two in-vitro applications: label noise reduction and training data expansion. Both applications resulted in statistically significant performance gains over baseline methods. The proposed framework outperformed state-of-the-art methods, achieving new benchmarks on a public empathy dataset with a Pearson correlation coefficient (PCC) of 0.648, among other metrics. Beyond the empirical results, this paper contributes a critical rethinking of evaluation practices in empathy computing, advocating for the adoption of the concordance correlation coefficient (CCC). The novel scale-aware prompting technique introduced here ensures alignment between LLM annotations and theoretical annotation protocols. We further highlight biases in the dataset across different demographic groups. Similar to the empathy detection dataset addressed in this paper, many other tasks, such as detecting depression, anxiety and mental health conditions, rely on questionnaire-based self-annotations. The proposed approach, therefore, opens exciting avenues for leveraging LLMs as complementary tools to enhance model training across different domains.', 'source': 'D:\\\\GitHub\\\\ArXiv_Info_System\\\\data\\\\raw\\\\html\\\\2501.00691v2.html', 'Header_1': 'Labels Generated by Large Language Models Help Measure People’s Empathy in Vitro', 'Header_2': 'V Conclusion', 'Header_3': None}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Тестовый поиск (берем вектор из датасета для примера)\n",
    "test_vector = df.iloc[0][\"embedding\"].astype(np.float32).tolist()\n",
    "\n",
    "hits = client.query_points(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query=test_vector,\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "print(\"\\nТестовый поиск (Top-3):\")\n",
    "for hit in hits.points:\n",
    "    print(f\"ID: {hit.id}, Score: {hit.score}\")\n",
    "    if hit.payload:\n",
    "        print(f\"Payload: {hit.payload}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add44319-c1b0-4548-8e8d-a8578a937b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab3e30d-fc7d-4837-80e4-f0bdbce255bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_vector = np.random.uniform(low=-1.0, high=1.0, size=(VECTOR_SIZE,)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f8bae1f-9a11-4fb2-9f58-6c6bc2ae9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = client.query_points(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query=synthetic_vector,\n",
    "    limit=5,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acec3b41-69a5-47e0-bdaa-76ec328dae15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResponse(points=[ScoredPoint(id=502714, version=1006, score=0.14965864, payload={'text': \"What makes an entity salient in discourse? > 2 Defining salience > 2.3 Graded summary-based salience: Boswijk and Coler ([2020](https://arxiv.org/html/2508.16464v1#bib.bib5); 713) point out that ``even when definitions [of salience] are given, they are often circular'' – the main issue issue responsible for this circularity in their analysis is reliance on text-internal criteria to identify salient mentions (e.g.subjecthood in Centering Theory or definiteness in the Mental Salience framework). These criteria interfere with, and in some cases preclude using corpus data to identify correlates of salience, as these may end up being the very features that define the term in that approach. Similarly, markedness or surprisal based approaches, while not tied to a specific marker such as definite articles or pronouns, also fall into the class of text-based definitions, because they are a direct function of word choice (pronouns are more frequent, etc.), rather than the result of human interpretation of the content.  \\nOur approach follows ideas first promoted by Dunietz and Gillick ([2014](https://arxiv.org/html/2508.16464v1#bib.bib15)) and developed further by Lin and Zeldes ([2024](https://arxiv.org/html/2508.16464v1#bib.bib34)), relying on summary-worthiness as the main criterion for salience. The basic idea of these approaches is that, because summaries are naturally constrained to be short, only the most salient entities will be mentioned in them. Put inversely, if an entity is truly salient in a document, then it should be difficult or impossible to summarize its content without mentioning it. This approach has several advantages:  \\n1. 1.  \\nIt is extra-textual, since the criterion for being salient does not depend directly on textual forms in the original document\\n2. 2.  \\nIt is applicable to any text type (unlike relying on hyperlinks, click-data, etc. which are exclusive to Web data)\\n3. 3.  \\nGiven a summary and a list of entities in a document, it is relatively easy to operationalize objectively whether each entity appears in the summary  \\nAt the same time, the approach also has several potential pitfalls:  \\n1. 1.\", 'source': 'D:\\\\GitHub\\\\ArXiv_Info_System\\\\data\\\\raw\\\\html\\\\2508.16464v1.html', 'Header_1': 'What makes an entity salient in discourse?', 'Header_2': '2 Defining salience', 'Header_3': '2.3 Graded summary-based salience'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=185755, version=372, score=0.13898133, payload={'text': 'Taxonomizing Representational Harms using Speech Act Theory > 7 Acknowledgments: We thank Solon Barocas, Su Lin Blodgett, Tim Vieira, Philip Resnik and his students, and many others who have shaped this paper over the years. We also thank our reviewers for their feedback.', 'source': 'D:\\\\GitHub\\\\ArXiv_Info_System\\\\data\\\\raw\\\\html\\\\2504.00928v2.html', 'Header_1': 'Taxonomizing Representational Harms using Speech Act Theory', 'Header_2': '7 Acknowledgments', 'Header_3': None}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=21258, version=43, score=0.13468371, payload={'text': 'TAD-Bench: A Comprehensive Benchmark for Embedding-Based Text Anomaly Detection > Appendix F Comparative Analysis of Anomaly Detection Algorithms: Anomaly detection algorithms vary in their underlying assumptions, computational efficiency, and effectiveness across different types of data distributions. In this section, we provide a comparative analysis of the eight anomaly detection methods used in this study: kNN, OCSVM, iForest, LOF, HBOS, ECOD, INNE and COPOD.  \\nDistance-based methods, such as kNN, define anomalies based on their relative distance to surrounding points. kNN anomaly detection computes the distance between a data point and its $k$ th nearest neighbor, with larger distances indicating potential anomalies. This method is conceptually simple and effective in low-dimensional spaces with clear separation between normal and anomalous points. However, its primary drawback is the curse of dimensionality, where distance metrics lose discriminative power as dimensionality increases. Additionally, kNN is computationally expensive, with a worst-case complexity of $O(n^{2})$ , making it impractical for large datasets without optimizations such as approximate nearest neighbor search.  \\nDensity-based approaches assume that anomalies reside in low-density regions relative to normal points. LOF estimates the local density of a point by comparing it with the densities of its neighbors. It is highly effective in detecting anomalies in datasets with non-uniform density distributions, where global models may fail. However, LOF is computationally expensive complexity $O(n^{2})$ in the worst case and sensitive to the choice of neighborhood size, requiring careful hyperparameter tuning.  \\nA more efficient density estimation approach is HBOS, which models feature distributions independently using histograms. This makes it computationally extremely fast $O(n)$ and scalable to large datasets. However, HBOS assumes feature independence, limiting its effectiveness when strong feature correlations exist. In such cases, its effectiveness diminishes as it fails to capture intricate relationships between features, potentially leading to suboptimal anomaly detection performance.  \\nIsolation-based approaches, such as iForest, take a different perspective by recursively partitioning the feature space. Since anomalies are typically isolated with fewer splits, iForest identifies them based on the depth required to isolate each point. iForest is computationally efficient $O(nlogn)$ and performs well in high-dimensional spaces compared to distance-based methods, but it is struggle with local anomalies. An extension of iForest, INNE, replaces axis-aligned splits with hypersphere-based partitions. This enhances robustness in detecting anomalies in complex distributions, particularly local anomalies.', 'source': 'D:\\\\GitHub\\\\ArXiv_Info_System\\\\data\\\\raw\\\\html\\\\2501.11960v2.html', 'Header_1': 'TAD-Bench: A Comprehensive Benchmark for Embedding-Based Text Anomaly Detection', 'Header_2': 'Appendix F Comparative Analysis of Anomaly Detection Algorithms', 'Header_3': None}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=566235, version=1133, score=0.13237993, payload={'text': 'Document Summarization with Conformal Importance Guarantees > 1 Introduction: Conformal prediction [[62](https://arxiv.org/html/2509.20461v1#bib.bib62), [59](https://arxiv.org/html/2509.20461v1#bib.bib59)] has recently risen in popularity as it provides distribution-free, finite-sample coverage guarantees [[2](https://arxiv.org/html/2509.20461v1#bib.bib2)], and has shown promise in classification [[56](https://arxiv.org/html/2509.20461v1#bib.bib56), [3](https://arxiv.org/html/2509.20461v1#bib.bib3), [33](https://arxiv.org/html/2509.20461v1#bib.bib33)], regression [[10](https://arxiv.org/html/2509.20461v1#bib.bib10), [43](https://arxiv.org/html/2509.20461v1#bib.bib43), [55](https://arxiv.org/html/2509.20461v1#bib.bib55)], and language tasks such as factual question answering [[37](https://arxiv.org/html/2509.20461v1#bib.bib37), [52](https://arxiv.org/html/2509.20461v1#bib.bib52), [45](https://arxiv.org/html/2509.20461v1#bib.bib45), [14](https://arxiv.org/html/2509.20461v1#bib.bib14), [23](https://arxiv.org/html/2509.20461v1#bib.bib23)]. In this work, we introduce Conformal Importance Summarization, the first application of conformal prediction to document summarization which provides statistical guarantees on the inclusion of important content.  \\nOur contributions are as follows:  \\n* •', 'source': 'D:\\\\GitHub\\\\ArXiv_Info_System\\\\data\\\\raw\\\\html\\\\2509.20461v1.html', 'Header_1': 'Document Summarization with Conformal Importance Guarantees', 'Header_2': '1 Introduction', 'Header_3': None}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=225359, version=451, score=0.13235375, payload={'text': 'Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories > 3 Counterspeech Against Conspiracy Theories: Conspiracy theories\\nare interpretative frameworks attempting to explain events as the covert actions of powerful, malicious groups or individuals pursuing self-interest at public expense Aaronovitch ([2010](https://arxiv.org/html/2504.16604v2#bib.bib1)); Byford ([2011](https://arxiv.org/html/2504.16604v2#bib.bib10)); Keeley ([1999](https://arxiv.org/html/2504.16604v2#bib.bib33)), overlooking actual, more intricate causes Popper and Kiesewetter ([2003](https://arxiv.org/html/2504.16604v2#bib.bib51)).\\nThey thus usually need actors (e.g., ‘globalists’) with a malicious goal (e.g., destabilizing national economies) through an action or strategy (e.g., manipulating weather) carried out in secrecy Samory and Mitra ([2018](https://arxiv.org/html/2504.16604v2#bib.bib57)). On social media, these narratives often appear in fragmented forms, with some components omitted or implied Steffen etal. ([2023](https://arxiv.org/html/2504.16604v2#bib.bib61)).\\nWhile CTs can overlap with HS Baider ([2023](https://arxiv.org/html/2504.16604v2#bib.bib5)), they have their own unique characteristics with regard to function and linguistic manifestation Hay ([2020](https://arxiv.org/html/2504.16604v2#bib.bib29)).', 'source': 'D:\\\\GitHub\\\\ArXiv_Info_System\\\\data\\\\raw\\\\html\\\\2504.16604v2.html', 'Header_1': 'Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories', 'Header_2': '3 Counterspeech Against Conspiracy Theories', 'Header_3': None}, vector=None, shard_key=None, order_value=None)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0da29-132b-4eed-92ae-26cb29600719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
