{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad434bc-55c1-4dde-b10a-e0dbb012d89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aaa6f92-ada0-408c-b220-90e00800f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === КОНФИГ ===\n",
    "INDEX_PATH = \"../data/processed/faiss_index/index.faiss\"\n",
    "CHUNKS_PATH = \"../data/processed/chunks.jsonl\"\n",
    "\n",
    "EMBED_MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "LLM_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "TOP_K = 10\n",
    "\n",
    "# ЗАПРОС ПОЛЬЗОВАТЕЛЯ\n",
    "QUERY = \"Which scientific papers explore graphs within the biomedical domain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f50e20-8c02-493f-8a64-7ce1ea0e1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer(EMBED_MODEL, device=device) \n",
    "index = faiss.read_index(INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825d74d5-f3c9-4aa1-a248-d7807993b023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Searching for: 'Which scientific papers explore graphs within the biomedical domain?' ---\n",
      "Top-10 Indices: [ 988 1362 1376 2084 2118 1374 3725 3673 1375 1364]\n",
      "Distances: [0.552, 0.544, 0.535, 0.533, 0.53, 0.527, 0.525, 0.524, 0.523, 0.508]\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Searching for: '{QUERY}' ---\")\n",
    "\n",
    "query_vector = embed_model.encode(\n",
    "    [QUERY], \n",
    "    prompt_name=\"query\", \n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "distances, indices = index.search(query_vector.astype('float32'), TOP_K)\n",
    "\n",
    "print(f\"Top-{TOP_K} Indices: {indices[0]}\")\n",
    "print(f\"Distances: {[round(x, 3) for x in distances[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ab59aa-d2ae-4e3f-9534-ab5207ca66b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fetching Chunks ---\n",
      "Retrieved 10 chunks.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Решить проблему с переполнением RAM\n",
    "print(\"--- Fetching Chunks ---\")\n",
    "\n",
    "with open(CHUNKS_PATH, 'r', encoding='utf-8') as f:\n",
    "    all_chunks = [json.loads(line) for line in f]\n",
    "\n",
    "top_indices = indices[0]\n",
    "top_chunks = [all_chunks[idx] for idx in top_indices if idx < len(all_chunks)]\n",
    "\n",
    "print(f\"Retrieved {len(top_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16be1ae-e707-444f-9a42-ab7119a6d2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Length (chars): 12712\n"
     ]
    }
   ],
   "source": [
    "def format_chunk_for_llm(chunk):\n",
    "    return (\n",
    "        f\"--- DOCUMENT ID: {chunk['id']} ---\\n\"\n",
    "        f\"TITLE: {chunk['metadata'].get('Title', 'N/A')}\\n\"\n",
    "        f\"CONTENT:\\n{chunk['text'].strip()}\\n\"\n",
    "    )\n",
    "\n",
    "context_block = \"\\n\\n\".join([format_chunk_for_llm(c) for c in top_chunks])\n",
    "\n",
    "print(f\"Context Length (chars): {len(context_block)}\")\n",
    "# print(context_block[:500] + \"...\") # Для дебага"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83347e9b-621b-4c58-bf71-1fa59c144596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.01 GB\n",
      "Memory reserved: 0.02 GB\n"
     ]
    }
   ],
   "source": [
    "# Освобождаем VRAM\n",
    "del embed_model\n",
    "del query_vector\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f8cba1-e7e8-432a-bbdc-53102d17b994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading LLM: Qwen/Qwen2.5-1.5B-Instruct ---\n",
      "LLM Loaded.\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Loading LLM: {LLM_MODEL} ---\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "print(\"LLM Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19dd7337-6c8f-4e0e-aa37-35d1e4574840",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"You are a Professional NLP Research Assistant.\"\n",
    "\n",
    "user_message = f\"\"\"### CONTEXT:\n",
    "{context_block}\n",
    "\n",
    "===\n",
    "IMPORTANT RULES:\n",
    "1. Answer ONLY using the context above.\n",
    "2. If no answer, say strictly: \"I don't have enough information.\"\n",
    "3. Write a paper ID.\n",
    "===\n",
    "\n",
    "### QUESTION:\n",
    "{QUERY}\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_instruction},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13abfdb2-c5f1-4a83-b227-7d613fe64ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Response ---\n",
      "\n",
      "=== RESPONSE ===\n",
      "\n",
      "Based on the provided context, several scientific papers explore graphs within the biomedical domain:\n",
      "\n",
      "1. **Multimodal Contrastive Representation Learning in Augmented Biomedical Knowledge Graphs** - This paper introduces a multimodal approach that combines embeddings from specialized Language Models with Graph Contrastive Learning to enhance biomedical knowledge graphs.\n",
      "\n",
      "2. **Attending To Syntactic Information In Biomedical Event Extraction Via Graph Neural Networks** - This paper discusses methods for extracting events from biomedical text using graph neural networks, focusing on leveraging syntactic information embedded in dependency parsing graphs.\n",
      "\n",
      "3. **Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large Language Models in Medical Applications** - While primarily focused on retrieval-augmented generation, this paper mentions the use of structured graphs in their framework, suggesting interest in integrating graphs into biomedical applications.\n",
      "\n",
      "These papers collectively highlight the importance of graph structures and their application in various aspects of biomedical research, particularly in event extraction and knowledge graph construction.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Generating Response ---\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Обрезаем входные токены, чтобы оставить только ответ\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"\\n=== RESPONSE ===\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa933bc-fd4d-486b-ad74-ffa398917f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
