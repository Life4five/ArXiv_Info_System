{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6bb0c79-13ac-4a81-8f8e-25e679b0d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940afc66-44cc-4e7e-90f5-16641f7b027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScienceRAG:\n",
    "    def __init__(self, \n",
    "                 qdrant_host: str = \"localhost\", \n",
    "                 qdrant_port: int = 6333,\n",
    "                 collection_name: str = \"nlp2025_chunks\",\n",
    "                 embed_model: str = \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "                 llm_model: str = \"Qwen/Qwen2.5-1.5B-Instruct\"):\n",
    "        \n",
    "        self.collection_name = collection_name\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device.upper()}\")\n",
    "\n",
    "        # 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π\n",
    "        self.client = QdrantClient(qdrant_host, port=qdrant_port)\n",
    "        \n",
    "        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ (–≠–º–±–µ–¥–¥–∏–Ω–≥–∏)\n",
    "        print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞: {embed_model}...\")\n",
    "        self.encoder = SentenceTransformer(embed_model, trust_remote_code=True, device=\"cpu\")\n",
    "        \n",
    "        # 3. –ó–∞–≥—Ä—É–∑–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ (LLM)\n",
    "        print(f\"üß† –ó–∞–≥—Ä—É–∑–∫–∞ LLM: {llm_model}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model)\n",
    "        \n",
    "        # –ò–ó–ú–ï–ù–ï–ù–ò–Ø –ó–î–ï–°–¨:\n",
    "        # 1. –£–±—Ä–∞–ª–∏ device_map=\"auto\"\n",
    "        # 2. –°–º–µ–Ω–∏–ª–∏ bfloat16 –Ω–∞ float16\n",
    "        # 3. –Ø–≤–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ .to(self.device)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model,\n",
    "            torch_dtype=torch.float16, \n",
    "            attn_implementation=\"sdpa\" # –í–∫–ª—é—á–∞–µ–º –±—ã—Å—Ç—Ä–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (Scaled Dot Product Attention)\n",
    "        ).to(self.device)\n",
    "        print(\"–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!\\n\")\n",
    "\n",
    "    \n",
    "    def _retrieve(self, query: str, top_k: int = 5):\n",
    "        \"\"\"–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥: –ø–æ–∏—Å–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤\"\"\"\n",
    "        query_vector = self.encoder.encode(query, convert_to_numpy=True)\n",
    "        \n",
    "        search_result = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_vector,\n",
    "            limit=top_k,\n",
    "            with_payload=True\n",
    "        )\n",
    "        \n",
    "        # –£–ø—Ä–æ—â–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞\n",
    "        return [point.payload for point in search_result.points]\n",
    "\n",
    "\n",
    "    def _format_context(self, chunks) -> str:\n",
    "        \"\"\"–°–æ–±–∏—Ä–∞–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞\"\"\"\n",
    "        formatted_text = \"\"\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # –ê–¥–∞–ø—Ç–∏—Ä—É–π –∫–ª—é—á–∏ –ø–æ–¥ —Å–≤–æ–π payload JSON\n",
    "            title = chunk.get('title', 'Unknown Title')\n",
    "            text = chunk.get('text', chunk.get('abstract', '')) # fallback –µ—Å–ª–∏ –Ω–µ—Ç text\n",
    "            \n",
    "            formatted_text += f\"Document [{i+1}]\\nTitle: {title}\\nContent: {text}\\n\\n\"\n",
    "        return formatted_text\n",
    "\n",
    "\n",
    "    def answer(self, query: str, top_k: int = 5) -> str:\n",
    "            t0 = time.time()\n",
    "            \n",
    "            # --- –≠–¢–ê–ü 1: –†–ï–¢–†–ò–í ---\n",
    "            print(f\"üîç –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é...\")\n",
    "            retrieved_chunks = self._retrieve(query, top_k)\n",
    "            t1 = time.time()\n",
    "            print(f\"‚è±Ô∏è  Retrieval (Encode + Search): {t1 - t0:.4f} —Å–µ–∫\")\n",
    "            \n",
    "            if not retrieved_chunks:\n",
    "                return \"–ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\"\n",
    "    \n",
    "            # --- –≠–¢–ê–ü 2: –ü–û–î–ì–û–¢–û–í–ö–ê ---\n",
    "            context = self._format_context(retrieved_chunks)\n",
    "            system_prompt = (\n",
    "                \"You are a helpful scientific assistant. \"\n",
    "                \"Use the provided context to answer the user's question. \"\n",
    "                \"If the context doesn't contain the answer, admit it.\"\n",
    "            )\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"}\n",
    "            ]\n",
    "            \n",
    "            text_input = self.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            model_inputs = self.tokenizer([text_input], return_tensors=\"pt\").to(self.device)\n",
    "            t2 = time.time()\n",
    "            \n",
    "            # --- –≠–¢–ê–ü 3: –ì–ï–ù–ï–†–ê–¶–ò–Ø ---\n",
    "            print(\"‚úçÔ∏è  –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...\")\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    **model_inputs,\n",
    "                    max_new_tokens=512,\n",
    "                    temperature=0.3,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "            t3 = time.time()\n",
    "            print(f\"‚è±Ô∏è  Generation (LLM): {t3 - t2:.4f} —Å–µ–∫\")\n",
    "    \n",
    "            response = self.tokenizer.batch_decode(\n",
    "                [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)],\n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            print(f\"‚è±Ô∏è  Total: {t3 - t0:.2f} —Å–µ–∫\")\n",
    "            return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cadd1a1-91b6-4cf0-8a98-5014600f5aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: CUDA\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞: Qwen/Qwen3-Embedding-0.6B...\n",
      "üß† –ó–∞–≥—Ä—É–∑–∫–∞ LLM: Qwen/Qwen2.5-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!\n",
      "\n",
      "üîç –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é...\n",
      "‚è±Ô∏è  Retrieval (Encode + Search): 0.2322 —Å–µ–∫\n",
      "‚úçÔ∏è  –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...\n",
      "‚è±Ô∏è  Generation (LLM): 10.5557 —Å–µ–∫\n",
      "‚è±Ô∏è  Total: 10.80 —Å–µ–∫\n",
      "\n",
      "========================================\n",
      "OTBET:\n",
      "Graph Neural Networks (GNNs) are used in drug discovery through various applications such as:\n",
      "\n",
      "1. **Drug Target Binding Affinity Prediction**: The Hybrid Graph-Transformer framework introduced by Xiao et al. ([2024]) uses GNNs to integrate both graph-based and sequence-based representations, achieving superior performance compared to state-of-the-art methods on benchmark datasets.\n",
      "\n",
      "2. **Multi-Objective Molecule Optimization**: The Latent Prompt Transformer developed by Kong et al. ([2024]) incorporates latent prompts within a unified architecture to achieve state-of-the-art performance in multi-objective molecule optimization and drug-like molecule generation.\n",
      "\n",
      "These applications leverage the ability of GNNs to process and analyze complex structures like molecules, making them valuable tools in advancing our understanding and development of new drugs.\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "rag = ScienceRAG()\n",
    "\n",
    "# –ó–∞–ø—Ä–æ—Å\n",
    "user_query = \"How are Graph Neural Networks used in drug discovery?\"\n",
    "\n",
    "answer = rag.answer(user_query, top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"OTBET:\")\n",
    "print(answer)\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b054be0b-49cc-49f5-8960-d0d50f6811f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é...\n",
      "‚è±Ô∏è  Retrieval (Encode + Search): 0.2035 —Å–µ–∫\n",
      "‚úçÔ∏è  –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...\n",
      "‚è±Ô∏è  Generation (LLM): 7.5100 —Å–µ–∫\n",
      "‚è±Ô∏è  Total: 7.72 —Å–µ–∫\n",
      "\n",
      "========================================\n",
      "OTBET:\n",
      "Based on the information provided in Document [4], Graph Neural Networks (GNNs) are being utilized in the field of knowledge-graph-guided language understanding, particularly within collaborative multi-agent question answering systems. Specifically, they are described as \"large-scale pretrained GNN backbones\" that aim to capture broadly reusable structural/semantic patterns. This suggests their application in generating synthetic graphs from text data, which can then be used to inform or enhance language model routing decisions. The use of GNNs here appears to leverage their ability to process relational data effectively, making them suitable for tasks involving complex interactions between entities represented as nodes in a graph structure.\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–ø—Ä–æ—Å\n",
    "user_query = \"How are Graph Neural Networks used in social media?\"\n",
    "\n",
    "answer = rag.answer(user_query, top_k=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"OTBET:\")\n",
    "print(answer)\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78070b24-5e47-40c8-8933-a812e9facc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
