{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0dbdc5-9e56-4139-8c52-bfa65966c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Твоя проверенная логика заголовков\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "def chunk_batch(batch):\n",
    "    all_chunks = []\n",
    "    all_parent_ids = []\n",
    "    all_metadata = []\n",
    "\n",
    "    for i in range(len(batch['markdown'])):\n",
    "        doc_id = batch['doc_id'][i]\n",
    "        text = batch['markdown'][i]\n",
    "        \n",
    "        # 1. Сплит по заголовкам\n",
    "        header_splits = header_splitter.split_text(text)\n",
    "        \n",
    "        # 2. Рекурсивный сплит\n",
    "        final_splits = text_splitter.split_documents(header_splits)\n",
    "        \n",
    "        for doc in final_splits:\n",
    "            all_chunks.append(doc.page_content)\n",
    "            all_parent_ids.append(doc_id)\n",
    "            # Сериализуем метаданные в строку, чтобы Parquet их \"переварил\"\n",
    "            all_metadata.append(json.dumps(doc.metadata)) \n",
    "\n",
    "    return {\n",
    "        \"chunk_text\": all_chunks, \n",
    "        \"parent_doc_id\": all_parent_ids,\n",
    "        \"section_metadata\": all_metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007f5bd-e8eb-4539-8034-789147e8eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. Загрузка данных из обработанного Parquet\n",
    "# Ты упомянул, что работаешь с Yandex Object Storage и S3, \n",
    "# поэтому путь должен указывать на локальную копию, синхронизированную через DVC\n",
    "ds = load_dataset(\"parquet\", data_files=\"../data/processed/texts.parquet\", split=\"train\")\n",
    "\n",
    "# 2. Запуск чанкования\n",
    "# Благодаря выносу в .py файл, это будет летать на всех ядрах без ошибок импорта\n",
    "chunked_ds = ds.map(\n",
    "    chunk_batch,\n",
    "    batched=True,\n",
    "    num_proc=os.cpu_count(),\n",
    "    remove_columns=ds.column_names\n",
    ")\n",
    "\n",
    "# 3. Сохранение финальных чанков\n",
    "# Эти данные пойдут на Runpod для генерации эмбеддингов\n",
    "chunked_ds.to_parquet(\"../data/processed/chunks.parquet\")\n",
    "\n",
    "print(f\"Подготовлено чанков: {len(chunked_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f38e1-a512-4513-a811-b30bc4e17c83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
