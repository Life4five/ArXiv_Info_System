# rag_pipeline.py
import torch
import time
import gc
import logging
from typing import List, Dict, Optional
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ScienceRAG:
    def __init__(self, 
                 qdrant_host: str = "localhost", 
                 qdrant_port: int = 6333,
                 collection_name: str = "nlp2025_chunks",
                 embed_model: str = "Qwen/Qwen3-Embedding-0.6B",
                 llm_model: str = "Qwen/Qwen2.5-1.5B-Instruct"):
        
        self.collection_name = collection_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"üöÄ Initializing RAG on device: {self.device.upper()}")

        # 1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant
        try:
            self.client = QdrantClient(qdrant_host, port=qdrant_port)
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collections = self.client.get_collections().collections
            collection_names = [c.name for c in collections]
            
            if collection_name not in collection_names:
                raise ValueError(
                    f"Collection '{collection_name}' not found. "
                    f"Available: {collection_names}"
                )
            logger.info(f"‚úÖ Connected to Qdrant collection: {collection_name}")
        except Exception as e:
            logger.error(f"‚ùå Failed to connect to Qdrant: {e}")
            raise
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ Encoder (–Ω–∞ CPU –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM)
        try:
            logger.info(f"üì• Loading retriever: {embed_model}...")
            self.encoder = SentenceTransformer(
                embed_model, 
                trust_remote_code=True, 
                device="cpu"
            )
            logger.info("‚úÖ Retriever loaded on CPU")
        except Exception as e:
            logger.error(f"‚ùå Failed to load encoder: {e}")
            raise
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π LLM
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # 3. –ó–∞–≥—Ä—É–∑–∫–∞ LLM (–Ω–∞ GPU —Å float16)
        try:
            logger.info(f"üß† Loading LLM: {llm_model}...")
            self.tokenizer = AutoTokenizer.from_pretrained(llm_model)
            self.model = AutoModelForCausalLM.from_pretrained(
                llm_model,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                attn_implementation="sdpa" if self.device == "cuda" else None,
                low_cpu_mem_usage=True
            ).to(self.device)
            logger.info(f"‚úÖ LLM loaded on {self.device.upper()}")
        except Exception as e:
            logger.error(f"‚ùå Failed to load LLM: {e}")
            raise
        
        logger.info("‚úÖ RAG system ready!\n")

    def _retrieve(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ payload'–æ–≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        try:
            # –≠–Ω–∫–æ–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ (–Ω–∞ CPU)
            query_vector = self.encoder.encode(
                query, 
                convert_to_numpy=True,
                show_progress_bar=False
            )
            
            # –ü–æ–∏—Å–∫ –≤ Qdrant
            search_result = self.client.query_points(
                collection_name=self.collection_name,
                query=query_vector.tolist(),
                limit=top_k,
                with_payload=True,
                with_vectors=False
            )
            
            return [point.payload for point in search_result.points]
        
        except Exception as e:
            logger.error(f"‚ùå Retrieval error: {e}")
            return []

    def _format_context(self, chunks: List[Dict]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM.
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ payload'–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            
        Returns:
            –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        if not chunks:
            return "No relevant documents found."
        
        formatted_text = ""
        for i, chunk in enumerate(chunks):
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π
            title = chunk.get('title', 'Unknown Title')
            text = chunk.get('text') or chunk.get('abstract') or chunk.get('content', 'No content')
            
            formatted_text += (
                f"Document [{i+1}]\n"
                f"Title: {title}\n"
                f"Content: {text}\n\n"
            )
        
        return formatted_text

    def _extract_sources(self, chunks: List[Dict], max_text_len: int = 200) -> List[Dict[str, str]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞.
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ payload'–æ–≤
            max_text_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å title –∏ text
        """
        sources = []
        for chunk in chunks:
            title = chunk.get("title", "Unknown")
            text = chunk.get("text") or chunk.get("abstract") or chunk.get("content", "")
            
            # –û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–µ–≤—å—é
            preview = text[:max_text_len] + "..." if len(text) > max_text_len else text
            
            sources.append({
                "title": title,
                "text": preview
            })
        
        return sources

    def answer(self, query: str, top_k: int = 5) -> Dict[str, any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞.
        
        Args:
            query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ 'answer' –∏ 'sources'
        """
        logger.info(f"üîç Processing query: '{query[:50]}...'")
        
        # 1. Retrieval
        retrieved_chunks = self._retrieve(query, top_k)
        
        if not retrieved_chunks:
            logger.warning("No documents found")
            return {
                "answer": "I couldn't find any relevant information in the knowledge base.",
                "sources": []
            }
        
        # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        sources = self._extract_sources(retrieved_chunks)
        
        # 3. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = self._format_context(retrieved_chunks)
        
        # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞
        system_prompt = (
            "You are a helpful scientific assistant with knowledge base of NLP arxiv paper for 2025 year. "
            "Use ONLY the provided context to answer the user's question."
            "If the context doesn't contain enough information, say so explicitly."
        )
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}
        ]

        # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        try:
            text_input = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            model_inputs = self.tokenizer(
                [text_input], 
                return_tensors="pt",
                truncation=True,
                max_length=4096
            ).to(self.device)

            logger.info("‚úçÔ∏è  Generating answer...")
            
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **model_inputs,
                    max_new_tokens=512,
                    temperature=0.3,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            response_text = self.tokenizer.batch_decode(
                [output_ids[len(input_ids):] 
                 for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)],
                skip_special_tokens=True
            )[0]
            
            # –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if self.device == "cuda":
                del model_inputs, generated_ids
                torch.cuda.empty_cache()
            
            logger.info("‚úÖ Answer generated successfully")
            
            return {
                "answer": response_text.strip(),
                "sources": sources
            }
        
        except Exception as e:
            logger.error(f"‚ùå Generation error: {e}")
            return {
                "answer": f"Error generating answer: {str(e)}",
                "sources": sources
            }

    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"""
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'encoder'):
            del self.encoder
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        logger.info("üßπ Resources cleaned up")
