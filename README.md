# ArXiv Info System

Вопросно-ответная RAG-система (RAG – Retrieval-Augmented Generation) по научным статьям с [arxiv.org](https://arxiv.org/).

**Автор проекта:** [Демидов Константин](https://github.com/ConstDemi)  
**Руководитель проекта:** [Паточенко Евгений](https://github.com/evgpat)


## Структура проекта

```text
info.ipynb               Паспорт проекта (описание целей, методологии, прогресса)

src/
├── metadata_parse.ipynb     Сбор метаданных статей в датафрейм
└── data_parse.ipynb         Скачивание статей на основе созданного датафрейма

data/                    Рабочее хранилище данных проекта (создастся при запуске data_parse.ipynb)
├── metadata/            Метаданные собранных статей
└── raw/                 "Сырые" (необработанные) статьи
    └── tex/             Архивы LaTeX-исходников статей (.tar.gz)
```

# Парсинг данных

Сбор корпуса научных статей из arXiv по категории **cs.CL** (Computation and Language) за 2025 год.

## Этап 1: Сбор метаданных

**Файл:** `metadata_parse.ipynb`

Собираем метаданные всех статей через arXiv API:

```python
# Параметры
CAT = 'cs.CL'
YEAR = 2025
```

### Особенности реализации

- **Помесячный сбор** — обход лимита API (10000 результатов)
- **Фильтрация по primary_category** — исключаем cross-listed статьи
- **Rate limiting** — задержка 3 секунды между запросами
- **Retry logic** — 3 попытки при ошибках

### Результат

```
Всего найдено: 22 891 статей
С primary category cs.CL: 16 503 статьи
```

**Выходные файлы:**
- `data/metadata/papers_metadata.csv`

---

## Этап 2: Загрузка статей

**Файл:** `data_parse.ipynb`

Скачиваем LaTeX-исходники статей:

### Процесс

1. **Проверка существующих файлов** — пропускаем уже скачанное
2. **Многопоточная загрузка** — 12 параллельных потоков
3. **Обработка ошибок** — логирование недоступных версий (HTTP 404)

### Результат

```
Всего в датасете: 16 503
Скачано успешно: 16 412 (.tar.gz архивов)
Ошибок: 91 (удалённые статьи)
```

**Выходная директория:**
```
data/raw/tex/
├── 2501.00001v1.tar.gz
├── 2501.00002v1.tar.gz
└── ...
```

---

## Итоговый датасет

- **Размер:** ~43 GB (16 412 архивов)
- **Формат:** LaTeX sources (.tar.gz)
- **Хранение:** Yandex Object Storage (DVC)

### Метаданные

Каждая статья содержит:
- `arxiv_id` — уникальный идентификатор
- `title` — название
- `authors` — список авторов
- `summary` — аннотация
- `categories` — категории arXiv
- `published` / `updated` — даты публикации
- `pdf_url` — ссылка на PDF

---

## Если хочется запустить проект локально:

### Предварительные требования
- Python 3.11+
- Git
- ~90 GB свободного места на диске

### Шаг 1: Клонирование репозитория
```bash
git clone https://github.com/ConstDemi/ArXiv_Info_System.git
cd ArXiv_Info_System
```

### Шаг 2: Установка зависимостей
```bash
pip install -r requirements.txt
```

### Шаг 3: Настройка доступа к хранилищу данных

1. Создайте файл `.dvc/config.local` на основе шаблона `.dvc/config.local.example`

2. Отредактируйте `.dvc/config.local`, заполнив переменные предоставленными credentials.

**Для членов комиссии:** credentials предоставляются отдельно.

### Шаг 4: Загрузка датасета
```bash
dvc pull
```

⚠️ **Внимание:** 
- Загрузка  данных может занять до 30 минут;
- Датасет весит 43 GB, но DVC ещё создаёт кэш. Итого 86 GB;
- Нотбуки из `src/` запускать не требуется - датасет уже собран и доступен через `dvc pull`.

### Шаг 5: Запуск RAG системы

*(в разработке)*
